/*
 * Tests restoring a single shard from a sharded cluster into an independent replica set.
 * Ensures that the shard identity document is correctly dropped.
 * The test does the following:
 *
 * - Starts a sharded cluster (with a single shard), inserts some initial data.
 * - Creates the backup data files, copies data files to the restore dbpath, computes dbHashes.
 * - Writes a restore configuration object to a named pipe via the mongo shell.
 * - Stops all nodes.
 * - Starts the nodes with --magicRestore (and "replicaSet") that parses the restore configuration
 * and exits cleanly.
 * - Restarts the nodes in the initial sharded cluster and asserts that the replica set config and
 * data are as expected, and that the dbHashes match.
 *
 * @tags: [
 *     requires_persistence,
 *     requires_wiredtiger,
 *     incompatible_with_windows_tls
 * ]
 */

import {ShardedMagicRestoreTest} from "jstests/libs/magic_restore_test.js";
import {isConfigCommitted} from "jstests/replsets/rslib.js";

function runTest(insertHigherTermOplogEntry) {
    jsTestLog("Running non-PIT magic restore with insertHigherTermOplogEntry: " +
              insertHigherTermOplogEntry);
    const numNodes = 2;
    // Setting priorities on the second node because assertConfigIsCorrect checks terms:
    // With only 2 nodes, it might happen that both nodes try to run for primary at the same time
    // and vote for themselves, which would increase the term.
    var st = new ShardingTest({
        shards: {
            rs0: {nodes: [{}, {rsConfig: {priority: 0}}]},
        },
        mongos: 1,
        config: [{}, {rsConfig: {priority: 0}}]
    });

    const dbName = "db";
    const coll = "coll";

    const db = st.getDB(dbName);
    jsTestLog("Inserting data to restore");  // This data will be reflected in the restored node.
    [-150, -50, 50, 150].forEach(
        val => { assert.commandWorked(db.getCollection(coll).insert({numForPartition: val})); });
    const expectedDocs = db.getCollection(coll).find().sort({numForPartition: 1}).toArray();
    assert.eq(expectedDocs.length, 4);

    const shardingRestoreTest = new ShardedMagicRestoreTest({
        st: st,
        pipeDir: MongoRunner.dataDir,
        insertHigherTermOplogEntry: insertHigherTermOplogEntry
    });

    jsTestLog("Taking checkpoints and opening backup cursors");
    shardingRestoreTest.takeCheckpointsAndOpenBackups();

    jsTestLog("Getting backup cluster dbHashes");
    // expected DBs are admin, config and db
    shardingRestoreTest.storePreRestoreDbHashes();

    // These documents will be truncated by magic restore, since they were written after the backup
    // cursor was opened.
    jsTestLog("Inserting data after backup cursor");
    [-151, -51, 51, 151].forEach(
        val => { assert.commandWorked(db.getCollection(coll).insert({numForPartition: val})); });
    assert.eq(db.getCollection(coll).find().toArray().length, 8);

    shardingRestoreTest.getShardRestoreTests().forEach((magicRestoreUtil) => {
        magicRestoreUtil.rst.nodes.forEach((node) => {
            // We inserted 8 documents and have 1 shard, so 8 per shard.
            magicRestoreUtil.assertOplogCountForNamespace(
                node, {ns: dbName + "." + coll, op: "i"}, 8);

            let {entriesAfterBackup} = magicRestoreUtil.getEntriesAfterBackup(node);
            // There might be operations after the backup from periodic jobs such as rangeDeletions
            // or ensureMajorityPrimaryAndScheduleDbTask, so we filter those out for the comparison
            // but still pass them into magic restore as additional oplog entries to apply.
            const filteredEntriesAfterBackup = entriesAfterBackup.filter(
                elem => (elem.ns != "config.rangeDeletions" &&
                         elem.o != "ensureMajorityPrimaryAndScheduleDbTask"));

            // Includes the 2 addOrRemoveShardInProgress entries generated by
            // transitionToDedicatedConfigServer.
            assert.eq(filteredEntriesAfterBackup.length,
                      4,
                      `filteredEntriesAfterBackup = ${
                          tojson(filteredEntriesAfterBackup)} is not of length 4`);
        });
    });

    shardingRestoreTest.findMaxCheckpointTsAndExtendBackupCursors();

    jsTestLog("Stopping all nodes");
    st.stop({noCleanData: true});

    // We restore the nodes from the shards except the config shard, in "replicaSet" mode.
    jsTestLog("Running Magic Restore");
    const magicRestoreUtil = shardingRestoreTest.shardRestoreTests[0];
    let restoreConfiguration = {
        "nodeType": "replicaSet",
        "replicaSetConfig": magicRestoreUtil.getExpectedConfig(),
        "maxCheckpointTs": magicRestoreUtil.getCheckpointTimestamp()
    };
    restoreConfiguration =
        magicRestoreUtil.appendRestoreToHigherTermThanIfNeeded(restoreConfiguration);

    magicRestoreUtil.writeObjsAndRunMagicRestore(
        restoreConfiguration, [], {"replSet": jsTestName() + "-rs"});

    jsTestLog("Starting restore replica set");
    // Restart the destination replica set.
    const rst = shardingRestoreTest.shardRestoreTests[0].rst;
    rst.startSet({
        dbpath: shardingRestoreTest.shardRestoreTests[0].getBackupDbPath(),
        noCleanData: true,
        replSet: jsTestName() + "-rs0"
    });

    rst.awaitNodesAgreeOnPrimary();
    // Make sure that all nodes have installed the config before moving on.
    let primary = rst.getPrimary();
    rst.waitForConfigReplication(primary);
    assert.soonNoExcept(() => isConfigCommitted(primary));

    for (let nodeIndex = 0; nodeIndex < numNodes; nodeIndex++) {
        const node = rst.nodes[nodeIndex];
        node.setSecondaryOk();

        const magicRestoreUtil = shardingRestoreTest.shardRestoreTests[0];
        const restoredDocs =
            node.getDB(dbName).getCollection(coll).find().sort({numForPartition: 1}).toArray();
        // The later 4 writes were truncated during magic restore. This test has a single shard.
        assert.eq(restoredDocs.length, 4);
        assert.eq(restoredDocs, expectedDocs);

        magicRestoreUtil.postRestoreChecks({
            node: node,
            dbName: dbName,
            collName: coll,
            expectedOplogCountForNs: 4,
            opFilter: "i",
            expectedNumDocsSnapshot: 4
        });

        // The shardIdentity document should be gone since we restored the shard as a replica set.
        let shardIdentity =
            node.getDB("admin").getCollection("system.version").findOne({_id: "shardIdentity"});
        assert.eq(shardIdentity, null);
    }

    jsTestLog("Getting restore replicaSets dbHashes");
    // Excluding admin.system.version, config.shards, config.actionlog, config.rangeDeletions,
    // cache.databases, cache.collections and cache.chunks.db.coll.
    const excludedCollections = [
        "system.version",
        "shards",
        "actionlog",
        "clusterParameters",
        "rangeDeletions",
        "cache.databases",
        "cache.collections",
        `cache.chunks.${dbName}.${coll}`,
        "system.keys"  // Empty collection on shards, not empty on RS nodes or config shards.
    ];
    shardingRestoreTest.shardRestoreTests[0].checkPostRestoreDbHashes(excludedCollections);

    jsTestLog("Creating restore sharded cluster");
    st = new ShardingTest({
        shards: 0,
        mongos: 1,
    });

    for (let nodeIndex = 0; nodeIndex < numNodes; nodeIndex++) {
        rst.restart(nodeIndex, {shardsvr: ''});
    }

    // Since we restored as replica sets above, nodes are missing the shard identity document
    // after restore, so addShard is needed.
    rst.awaitReplication();
    assert.commandWorked(st.s.adminCommand({addShard: rst.getURL(), name: rst.name}));

    // Those are affected by the addShard so ignoring them, they matched above.
    excludedCollections.push("transactions", "system.sessions");
    shardingRestoreTest.shardRestoreTests[0].checkPostRestoreDbHashes(excludedCollections);

    jsTestLog("Stopping restore nodes");
    st.stop();
    rst.stopSet();
}

// Run non-PIT restore twice, with one run performing a no-op oplog entry insert with a higher term.
// This affects the stable timestamp on magic restore node shutdown.
runTest(false /* insertHigherTermOplogEntry */);
runTest(true /* insertHigherTermOplogEntry */);
